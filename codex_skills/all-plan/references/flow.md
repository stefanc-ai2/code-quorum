# All Plan (Codex Version)

Collaborative planning with mounted CLIs for comprehensive solution design. Codex serves as the primary coordinator.

**Usage**: For complex features or architectural decisions requiring diverse perspectives.

---

## Input Parameters

From `$ARGUMENTS`:
- `requirement`: User's initial requirement or feature request
- `context`: Optional project context or constraints

---

## Pre-Execution: Detect Mounted Providers

**CRITICAL**: Before starting the flow, determine which providers are actually available.

Run:
```bash
cq-mounted
```

This returns JSON like:
```json
{"cwd":"/path/to/project","mounted":["codex","claude"]}
```

Parse the `mounted` array and **ONLY dispatch to providers in this list**.

- If `"claude"` is mounted ‚Üí dispatch to Claude (Codex also designs independently)

**Skip any provider not in the mounted list.** Do not attempt to dispatch to unmounted providers.

Save the mounted providers list as `mounted_providers`.

---

## Execution Flow

### Phase 1: Requirement Refinement & Project Analysis

**1.1 Structured Clarification (Option-Based)**

Use the **5-Dimension Planning Readiness Model** to ensure comprehensive requirement capture.

#### Readiness Dimensions (100 pts total)

| Dimension | Weight | Focus | Priority |
|-----------|--------|-------|----------|
| Problem Clarity | 30pts | What problem? Why solve it? | 1 |
| Functional Scope | 25pts | What does it DO? Key features | 2 |
| Success Criteria | 20pts | How to verify done? | 3 |
| Constraints | 15pts | Time, resources, compatibility | 4 |
| Priority/MVP | 10pts | What first? Phased delivery? | 5 |

#### Clarification Flow

```
ROUND 1:
  1. Parse initial requirement
  2. Identify 2 lowest-confidence dimensions (use Priority order for ties)
  3. Present 2 questions with options (1 per dimension)
  4. User selects options
  5. Update dimension scores based on answers
  6. Display Scorecard to user

IF readiness_score >= 80: Skip Round 2, proceed to 1.2
ELSE:
  ROUND 2:
    1. Re-identify 2 lowest-scoring dimensions
    2. Ask 2 more questions
    3. Update scores
    4. Proceed regardless (with gap summary)

QUICK-START OVERRIDE:
  - User can select "Proceed anyway" at any point
  - All dimensions marked as "assumption" in summary
```

#### Option Bank Reference

**Problem Clarity (30pts)**
```
Question: "What type of problem are you solving?"
Options:
  A. "Specific bug or defect with clear reproduction" ‚Üí 27pts
  B. "New feature with defined business value" ‚Üí 27pts
  C. "Performance/optimization improvement" ‚Üí 24pts
  D. "General improvement or refactoring" ‚Üí 18pts
  E. "Not sure yet - need exploration" ‚Üí 9pts (flag)
  F. "Other: ___" ‚Üí 12pts (flag)
```

**Functional Scope (25pts)**
```
Question: "What is the scope of functionality?"
Options:
  A. "Single focused component/module" ‚Üí 23pts
  B. "Multiple related components" ‚Üí 20pts
  C. "Cross-cutting system change" ‚Üí 18pts
  D. "Unclear - need codebase analysis" ‚Üí 10pts (flag)
  E. "Other: ___" ‚Üí 10pts (flag)
```

**Success Criteria (20pts)**
```
Question: "How will you verify success?"
Options:
  A. "Automated tests (unit/integration/e2e)" ‚Üí 18pts
  B. "Performance benchmarks with targets" ‚Üí 18pts
  C. "Manual testing with checklist" ‚Üí 14pts
  D. "User feedback/acceptance" ‚Üí 12pts
  E. "Not defined yet" ‚Üí 6pts (flag)
  F. "Other: ___" ‚Üí 8pts (flag)
```

**Constraints (15pts)**
```
Question: "What are the primary constraints?"
Options:
  A. "Time-sensitive (deadline driven)" ‚Üí 14pts
  B. "Must maintain backward compatibility" ‚Üí 14pts
  C. "Resource/budget limited" ‚Üí 12pts
  D. "Security/compliance critical" ‚Üí 14pts
  E. "No specific constraints" ‚Üí 10pts
  F. "Other: ___" ‚Üí 8pts (flag)
```

**Priority/MVP (10pts)**
```
Question: "What is the delivery approach?"
Options:
  A. "MVP first, iterate later" ‚Üí 9pts
  B. "Full feature, single release" ‚Üí 9pts
  C. "Phased rollout planned" ‚Üí 9pts
  D. "Exploratory - scope TBD" ‚Üí 5pts (flag)
  E. "Other: ___" ‚Üí 5pts (flag)
```

#### Gap Classification Rules

| Dimension Score | Classification | Handling |
|-----------------|----------------|----------|
| ‚â•70% of weight | ‚úì Defined | Include in Design Brief |
| 50-69% of weight | ‚ö†Ô∏è Assumption | Carry forward as risk |
| <50% of weight | üö´ Gap | Flag in brief, may need validation |

Example thresholds:
- Problem Clarity: ‚â•21 Defined, 15-20 Assumption, <15 Gap
- Functional Scope: ‚â•18 Defined, 13-17 Assumption, <13 Gap
- Success Criteria: ‚â•14 Defined, 10-13 Assumption, <10 Gap
- Constraints: ‚â•11 Defined, 8-10 Assumption, <8 Gap
- Priority/MVP: ‚â•7 Defined, 5-6 Assumption, <5 Gap

#### Clarification Summary Output

After clarification, generate:

```
CLARIFICATION SUMMARY
=====================
Readiness Score: [X]/100

Dimensions:
- Problem Clarity: [X]/30 [‚úì/‚ö†Ô∏è/üö´]
- Functional Scope: [X]/25 [‚úì/‚ö†Ô∏è/üö´]
- Success Criteria: [X]/20 [‚úì/‚ö†Ô∏è/üö´]
- Constraints: [X]/15 [‚úì/‚ö†Ô∏è/üö´]
- Priority/MVP: [X]/10 [‚úì/‚ö†Ô∏è/üö´]

Assumptions & Gaps:
- [Dimension]: [assumption or gap description]
- [Dimension]: [assumption or gap description]

Proceeding to project analysis...
```

Save as `clarification_summary`.

**1.2 Analyze Project Context**

Use available tools to understand:
- Existing codebase structure (Glob, Grep, Read)
- Current architecture patterns
- Dependencies and tech stack
- Related existing implementations

**1.3 Research (if needed)**

If the requirement involves:
- New technologies or frameworks
- Industry best practices
- Performance benchmarks
- Security considerations

Use WebSearch to gather relevant information.

**1.4 Formulate Complete Brief**

Create a comprehensive design brief incorporating clarification results:

```
DESIGN BRIEF
============
Readiness Score: [X]/100

Problem: [clear problem statement]
Context: [project context, tech stack, constraints]

Requirements:
- [requirement 1]
- [requirement 2]
- [requirement 3]

Success Criteria:
- [criterion 1]
- [criterion 2]

Assumptions (from clarification):
- [assumption 1]
- [assumption 2]

Gaps to Validate:
- [gap 1]
- [gap 2]

Research Findings: [if applicable]
```

Save as `design_brief`.

---

### Phase 2: Parallel Design (Multi-Turn)

Send the design brief to mounted CLIs (from `mounted_providers`) for independent design.

**IMPORTANT**: Only dispatch to providers that appear in `mounted_providers`. Skip any provider not in the list.

**2.1 Dispatch to Claude** (if "claude" in mounted_providers)

Generate a stable 32-hex `req_id` as `CLAUDE_PLAN_REQ_ID`, then send the request via `ask` using that id:

```bash
CLAUDE_PLAN_REQ_ID="$(python -c 'import secrets; print(secrets.token_hex(16))')"

ask claude --req-id "$CLAUDE_PLAN_REQ_ID" <<'EOF'
You are participating in /all-plan. Reply with design feedback only (no code changes).

When done, send your design back to Codex via:
  ask codex --reply-to=<req_id> --caller claude "<your design>"

Design a solution for this requirement:

[design_brief]

Provide:
- Goal (1 sentence)
- Architecture approach
- Implementation steps (3-7 key steps)
- Technical considerations
- Potential risks
- Acceptance criteria (max 3)

Be specific and concrete.
EOF
```

Note: Don‚Äôt worry about how to get the reply yet ‚Äî just send the request and continue. You‚Äôll collect replies in Step 3.1 by ending your turn.

**2.4 Codex's Independent Design**

While waiting for responses, create YOUR own design (do not look at others yet):
- Goal (1 sentence)
- Architecture approach
- Implementation steps (3-7 key steps)
- Technical considerations
- Potential risks
- Acceptance criteria (max 3)

Save as `codex_design`.

---

### Phase 3: Collect & Analyze All Designs

**3.1 Collect Response(s)**

This flow is **multi-turn**. To collect responses: end your turn (do not run additional commands). Claude will send a message back to your terminal (driver pane) via `ask codex --reply-to=<CLAUDE_PLAN_REQ_ID> ...`.

- When the reply arrives, save it as `claude_design`.
- Your own design should already be saved as `codex_design`.
- Do not scrape panes to collect replies (forbidden): no `wezterm cli get-text`, no `tmux capture-pane`, etc. The only supported mechanism is reply-via-ask.

**3.2 Comparative Analysis**

Analyze designs from Codex + all mounted providers that responded:

Create a comparison matrix:
```
DESIGN COMPARISON
=================

1. Goals Alignment
   - Common goals across all designs
   - Unique perspectives from each

2. Architecture Approaches
   - Overlapping patterns
   - Divergent approaches
   - Pros/cons of each

3. Implementation Steps
   - Common steps (high confidence)
   - Unique steps (need evaluation)
   - Missing steps in some designs

4. Technical Considerations
   - Shared concerns
   - Unique insights from each CLI
   - Critical issues identified

5. Risk Assessment
   - Commonly identified risks
   - Unique risks from each perspective
   - Risk mitigation strategies

6. Acceptance Criteria
   - Overlapping criteria
   - Additional criteria to consider
```

Save as `comparative_analysis`.

---

### Phase 4: Iterative Refinement with Claude

**4.1 Draft Merged Design**

Based on comparative analysis, create initial merged design:
```
MERGED DESIGN (Draft v1)
========================
Goal: [synthesized goal]

Architecture: [best approach from analysis]

Implementation Steps:
1. [step 1]
2. [step 2]
3. [step 3]
...

Technical Considerations:
- [consideration 1]
- [consideration 2]

Risks & Mitigations:
- Risk: [risk 1] ‚Üí Mitigation: [mitigation 1]
- Risk: [risk 2] ‚Üí Mitigation: [mitigation 2]

Acceptance Criteria:
- [criterion 1]
- [criterion 2]
- [criterion 3]

Open Questions:
- [question 1]
- [question 2]
```

Save as `merged_design_v1`.

**4.2 Discussion Round 1 - Review & Critique**

```bash
CLAUDE_REVIEW_1_REQ_ID="$(python -c 'import secrets; print(secrets.token_hex(16))')"

ask claude --req-id "$CLAUDE_REVIEW_1_REQ_ID" <<'EOF'
You are participating in /all-plan. Reply with critique only (no code changes).

When done, send your review back to Codex via:
  ask codex --reply-to=<req_id> --caller claude "<your review>"

Review this merged design based on all CLI inputs:

COMPARATIVE ANALYSIS:
[comparative_analysis]

MERGED DESIGN v1:
[merged_design_v1]

Analyze:
1. Does this design capture the best ideas from all perspectives?
2. Are there any conflicts or contradictions?
3. What's missing or unclear?
4. Are the implementation steps logical and complete?
5. Are risks adequately addressed?

Provide specific recommendations for improvement.
EOF
```

This is **multi-turn**:
- Stop and wait (end your turn). Claude will reply via `ask codex --reply-to=<CLAUDE_REVIEW_1_REQ_ID> ...`.
- When it arrives, save it as `claude_review_1`.

**4.3 Discussion Round 2 - Resolve & Finalize**

Based on Claude's review, refine the design:

```bash
CLAUDE_REVIEW_2_REQ_ID="$(python -c 'import secrets; print(secrets.token_hex(16))')"

ask claude --req-id "$CLAUDE_REVIEW_2_REQ_ID" <<'EOF'
You are participating in /all-plan. Reply with final suggestions only (no code changes).

When done, send your response back to Codex via:
  ask codex --reply-to=<req_id> --caller claude "<your response>"

Refined design based on your feedback:

MERGED DESIGN v2:
[merged_design_v2]

Changes made:
- [change 1]
- [change 2]

Remaining concerns:
- [concern 1 if any]

Final approval or additional suggestions?
EOF
```

This is **multi-turn**:
- Stop and wait (end your turn). Claude will reply via `ask codex --reply-to=<CLAUDE_REVIEW_2_REQ_ID> ...`.
- When it arrives, save it as `claude_review_2`.

---

### Phase 5: Final Output

**5.1 Finalize Design**

Incorporate Claude's final feedback and create the complete solution design.

**5.2 Save Plan Document**

Write the final plan to a markdown file:

**File path**: `.cq_config/plans/{feature-name}-plan.md`

Use this template:

```markdown
# {Feature Name} - Solution Design

> Generated by all-plan collaborative design process (Codex-led)

## Overview

**Goal**: [Clear, concise goal statement]

**Readiness Score**: [X]/100

**Generated**: [Date]

---

## Requirements Summary

### Problem Statement
[Clear problem description]

### Scope
[What's in scope and out of scope]

### Success Criteria
- [ ] [criterion 1]
- [ ] [criterion 2]
- [ ] [criterion 3]

### Constraints
- [constraint 1]
- [constraint 2]

### Assumptions
- [assumption 1 from clarification]
- [assumption 2 from clarification]

---

## Architecture

### Approach
[Chosen architecture approach with rationale]

### Key Components
- **[Component 1]**: [description]
- **[Component 2]**: [description]

### Data Flow
[If applicable, describe data flow]

---

## Implementation Plan

### Step 1: [Title]
- **Actions**: [specific actions]
- **Deliverables**: [what will be produced]
- **Dependencies**: [what's needed first]

### Step 2: [Title]
- **Actions**: [specific actions]
- **Deliverables**: [what will be produced]
- **Dependencies**: [what's needed first]

### Step 3: [Title]
- **Actions**: [specific actions]
- **Deliverables**: [what will be produced]
- **Dependencies**: [what's needed first]

[Continue for all steps...]

---

## Technical Considerations

- [consideration 1]
- [consideration 2]
- [consideration 3]

---

## Risk Management

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|------------|
| [risk 1] | High/Med/Low | High/Med/Low | [strategy] |
| [risk 2] | High/Med/Low | High/Med/Low | [strategy] |

---

## Acceptance Criteria

- [ ] [criterion 1]
- [ ] [criterion 2]
- [ ] [criterion 3]

---

## Design Contributors

| CLI | Key Contributions |
|-----|-------------------|
| Codex | [contributions] |
[For each provider in mounted_providers, add a row:]
| [Provider] | [contributions] |

---

## Appendix

### Clarification Summary
[Include the clarification summary from Phase 1.1]

### Alternative Approaches Considered
[Brief notes on approaches that were evaluated but not chosen]
```

**5.3 Output to User**

After saving the file, display to user:

```
PLAN COMPLETE
=============

‚úì Plan saved to: .cq_config/plans/{feature-name}-plan.md

Summary:
- Goal: [1-sentence goal]
- Steps: [N] implementation steps
- Risks: [N] identified with mitigations
- Readiness: [X]/100

Next: Review the plan and proceed with implementation when ready.
```

---

## Principles

1. **Structured Clarification**: Use option-based questions to systematically capture requirements
2. **Readiness Scoring**: Quantify requirement completeness before proceeding
3. **True Independence**: Draft the driver‚Äôs design before reading/responding to other CLIs‚Äô designs
4. **Diverse Perspectives**: Leverage unique strengths of each CLI (Codex: code, Claude: context)
5. **Evidence-Based Synthesis**: Merge based on comparative analysis, not arbitrary choices
6. **Iterative Refinement**: Use Claude discussion to validate and improve merged design
7. **Concrete Deliverables**: Output actionable plan document, not just discussion notes
8. **Attribution**: Acknowledge contributions from each CLI to maintain transparency
9. **Research When Needed**: Don't hesitate to use WebSearch for external knowledge
10. **Max 2 Iteration Rounds**: Avoid endless discussion; converge on practical solution
11. **Document Output**: Always save final plan as markdown file

---

## Notes

- This skill is designed for complex features or architectural decisions
- For simple tasks, use dual-design or direct implementation instead
- This flow is multi-turn: dispatch via `ask`, then continue once reply-via-ask results arrive in-pane
- **CRITICAL**: Always run `cq-mounted` first to detect which providers are active. Only dispatch to providers in the `mounted` array
- If only Codex and Claude are mounted, the collaboration will be between those two only
- Plans are saved to `.cq_config/plans/` with descriptive filenames
